# Module 1 Homework: Docker & SQL

In this homework we'll prepare the environment and practice
Docker and SQL

When submitting your homework, you will also need to include
a link to your GitHub repository or other public code-hosting
site.

This repository should contain the code for solving the homework.

When your solution has SQL or shell commands and not code
(e.g. python files) file format, include them directly in
the README file of your repository.


## Question 1. Understanding Docker images

Run docker with the `python:3.13` image. Use an entrypoint `bash` to interact with the container.

What's the version of `pip` in the image?

- 25.3 ‚úÖ 
- 24.3.1
- 24.2.1
- 23.3.1
### Explanation: 
After running a docker container using this command:
```bash
docker run -it --rm --entrypoint=bash python:3.13-slim
```
and then inside the container running this command to get the pip version:
```bash
pip --version
#or
pip -V
```
I was able to get this output:
```bash
pip 25.3 from /usr/local/lib/python3.13/site-packages/pip (python 3.13)
```
## Question 2. Understanding Docker networking and docker-compose

Given the following `docker-compose.yaml`, what is the `hostname` and `port` that pgadmin should use to connect to the postgres database?

```yaml
services:
  db:
    container_name: postgres
    image: postgres:17-alpine
    environment:
      POSTGRES_USER: 'postgres'
      POSTGRES_PASSWORD: 'postgres'
      POSTGRES_DB: 'ny_taxi'
    ports:
      - '5433:5432'
    volumes:
      - vol-pgdata:/var/lib/postgresql/data

  pgadmin:
    container_name: pgadmin
    image: dpage/pgadmin4:latest
    environment:
      PGADMIN_DEFAULT_EMAIL: "pgadmin@pgadmin.com"
      PGADMIN_DEFAULT_PASSWORD: "pgadmin"
    ports:
      - "8080:80"
    volumes:
      - vol-pgadmin_data:/var/lib/pgadmin

volumes:
  vol-pgdata:
    name: vol-pgdata
  vol-pgadmin_data:
    name: vol-pgadmin_data
```

- postgres:5433
- localhost:5432
- db:5433
- postgres:5432 ‚úÖ
- db:5432 ‚úÖ

If multiple answers are correct, select any 
### Explanation:
After running the multi services docker container using this command:
```bash
docker-compose up
```
and then going to `localhost:8080` for the pgadmin web interface I was able to add the postgres server using both the service name `db` and the container_name `postgres` as the host name in pgadmin using `5432` as the port as that's the port that's exposed inside the container meanwhile `5433` is exposed in the host machine(our pc in this case).
## Prepare the Data

Download the green taxi trips data for November 2025:

```bash
wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2025-11.parquet
```

You will also need the dataset with zones:

```bash
wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv
```

## Question 3. Counting short trips

For the trips in November 2025 (lpep_pickup_datetime between '2025-11-01' and '2025-12-01', exclusive of the upper bound), how many trips had a `trip_distance` of less than or equal to 1 mile?

- 7,853
- 8,007 ‚úÖ
- 8,254
- 8,421
### Explanation:
After loading the dataset parquet using pandas and filtering through both columns `lpep_pickup_datetime` and `trip_distance`, the resulted Dataframe had a shape of (8007, 21). Check the notebook `homework.ipynb`for more details.


## Question 4. Longest trip for each day

Which was the pick up day with the longest trip distance? Only consider trips with `trip_distance` less than 100 miles (to exclude data errors).

Use the pick up time for your calculations.

- 2025-11-14 ‚úÖ
- 2025-11-20
- 2025-11-23
- 2025-11-25
### Explanation:
After selecting only the rows with less than 100 miles, sorting by the `trip_distance` column in descending order and selecting the first column, we got a Dataframe with `2025-11-14` as its `lpep_pickup_datetime` column. Check the notebook `homework.ipynb` for more details.

## Question 5. Biggest pickup zone

Which was the pickup zone with the largest `total_amount` (sum of all trips) on November 18th, 2025?

- East Harlem North ‚úÖ
- East Harlem South
- Morningside Heights
- Forest Hills
### Explanation:
After selecting only the trips on `2025-11-18` and then doing a group by each pickup zone by calculating the total sum of `total_amount` we were able to get a Dataframe with only the pickup zones (IDs) and their total amount, which we then can sort by the `total_amount` in descending order and select the first column, but the work doesn't end there just yet, as the Dataframe would only give us the pickup zone location ID. We can then take it and do a simple lookup in the other Dataframe that has each zone's details along with the full name, which in turn gave us for the locationID `74` the name of its zone`East Harlem North`. Check the notebook `homework.ipynb` for more details.

## Question 6. Largest tip

For the passengers picked up in the zone named "East Harlem North" in November 2025, which was the drop off zone that had the largest tip?

Note: it's `tip` , not `trip`. We need the name of the zone, not the ID.

- JFK Airport
- Yorkville West ‚úÖ
- East Harlem North
- LaGuardia Airport
### Explanation:
After selecting only the trips in the `East Harlem North` zone during November 2025 and then sorting by `tip_amount` in descending order, we were able to get the trip with the highest `tip_amount` which had a dropoff zone with locationID `263` which we can use to look up the full details of that zone in the zone lookup dataframe, which in turn gave us the name of the zone as `Yorkville West`. Check the notebook `homework.ipynb` for more details.

## Terraform

In this section homework we'll prepare the environment by creating resources in GCP with Terraform.

In your VM on GCP/Laptop/GitHub Codespace install Terraform.
Copy the files from the course repo
[here](../../../01-docker-terraform/terraform/terraform) to your VM/Laptop/GitHub Codespace.

Modify the files as necessary to create a GCP Bucket and Big Query Dataset.


## Question 7. Terraform Workflow

Which of the following sequences, respectively, describes the workflow for:
1. Downloading the provider plugins and setting up backend,
2. Generating proposed changes and auto-executing the plan
3. Remove all resources managed by terraform`

Answers:
- terraform import, terraform apply -y, terraform destroy
- teraform init, terraform plan -auto-apply, terraform rm
- terraform init, terraform run -auto-approve, terraform destroy
- terraform init, terraform apply -auto-approve, terraform destroy
- terraform import, terraform apply -y, terraform rm


## Submitting the solutions

* Form for submitting: https://courses.datatalks.club/de-zoomcamp-2026/homework/hw1


## Learning in Public

We encourage everyone to share what they learned. This is called "learning in public".

### Why learn in public?

- Accountability: Sharing your progress creates commitment and motivation to continue
- Feedback: The community can provide valuable suggestions and corrections
- Networking: You'll connect with like-minded people and potential collaborators
- Documentation: Your posts become a learning journal you can reference later
- Opportunities: Employers and clients often discover talent through public learning

You can read more about the benefits [here](https://alexeyondata.substack.com/p/benefits-of-learning-in-public-and).

Don't worry about being perfect. Everyone starts somewhere, and people love following genuine learning journeys!

### Example post for LinkedIn

```
üöÄ Week 1 of Data Engineering Zoomcamp by @DataTalksClub complete!

Just finished Module 1 - Docker & Terraform. Learned how to:

‚úÖ Containerize applications with Docker and Docker Compose
‚úÖ Set up PostgreSQL databases and write SQL queries
‚úÖ Build data pipelines to ingest NYC taxi data
‚úÖ Provision cloud infrastructure with Terraform

Here's my homework solution: <LINK>

Following along with this amazing free course - who else is learning data engineering?

You can sign up here: https://github.com/DataTalksClub/data-engineering-zoomcamp/
```

### Example post for Twitter/X


```
üê≥ Module 1 of Data Engineering Zoomcamp done!

- Docker containers
- Postgres & SQL
- Terraform & GCP
- NYC taxi data pipeline

My solution: <LINK>

Free course by @DataTalksClub: https://github.com/DataTalksClub/data-engineering-zoomcamp/
```


